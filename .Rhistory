foreach::getDoParRegistered()
foreach::getDoParWorkers()
message("Succcesfully finished parallelization")
# rvest loop
NSS_applied_judgments <- foreach(NSS_ID = NSS_IDs, .combine = "rbind", .packages = c("httr", "rvest", "rapportools", "foreach", "tidyverse", "lubridate", "xml2")) %dopar% {
html <- paste0("https://vyhledavac.nssoud.cz/DokumentDetail/Index/", NSS_ID) %>%
read_html()
length_applied_judgments <- length(html_elements(html, xpath = "//td[@title = 'prejudikaturaoznacenivecivcelku']/../../../../table/tbody/tr"))
doc_id <- html_elements(html, xpath ="//*[@class='det-textitle' and contains(@title, 'ECLI (ecli)')]/../*[@class='det-textval']")
if (length(doc_id) == 0) {
doc_id <- NA
} else {doc_id <- html_text(doc_id)}
if(length_applied_judgments == 0) {
applied_judgments_result <- NULL
} else {
applied_judgments_result <- foreach(i = seq(length_applied_judgments), .combine = "rbind") %do% {
inner_length <- length(html_elements(html, xpath = "//td[@title = 'prejudikaturaoznacenivecivcelku']/../../../../table/tbody/tr[1]/td"))
output <- tibble(
"doc_id" = doc_id,
"applied_judgment_case_id" = html_elements(html, xpath = paste0("//td[@title = 'prejudikaturaoznacenivecivcelku']/../../../../table/tbody/tr[",i,"]/td[1]")) %>% html_text2(),
"applied_judgment_authority" = html_elements(html, xpath = paste0("//td[@title = 'prejudikaturaoznacenivecivcelku']/../../../../table/tbody/tr[",i,"]/td[", inner_length-2,"]")) %>% html_text2(),
"applied_judgment_relationship" = html_elements(html, xpath = paste0("//td[@title = 'prejudikaturaoznacenivecivcelku']/../../../../table/tbody/tr[",i,"]/td[", inner_length-1,"]")) %>% html_text2(),
"applied_judgment_conflict" = html_elements(html, xpath = paste0("//td[@title = 'prejudikaturaoznacenivecivcelku']/../../../../table/tbody/tr[",i,"]/td[", inner_length, "]")) %>% html_text2()
)
return(output)
}
return(applied_judgments_result)
}
}
# Stop the parallel process
parallel::stopCluster(cl = my.cluster)
}
return(NSS_applied_judgments)
}
NSS_applied_judgments[[7]] <- get_applied_judgments(NSS_IDs = NSS_IDs[60001:69600], par = TRUE)
NSS_applied_judgments[[7]] <- get_applied_judgments(NSS_IDs = NSS_IDs[60001:69600], par = TRUE)
NSS_applied_judgments[[7]] <- get_applied_judgments(NSS_IDs = NSS_IDs[60001:69600], par = FALSE)
xfun::pkg_attach2("httr", "rvest", "progress", "rapportools", "foreach", "tidyverse", "lubridate", "xml2", "doParallel", "iterators")
# Load the data
source("supporting_functions.R")
load("data/NSS_IDs.RData")
load(file = "data/NSS_applied_judgments.RData")
get_applied_judgments <- function(NSS_IDs, par = FALSE) {
if (par == FALSE) {
pb <- progress_bar$new(
format = "  downloading [:bar] :current/:total :percent eta: :eta",
total = length(NSS_IDs), clear = FALSE, width = 80)
NSS_applied_judgments <- foreach(NSS_ID = NSS_IDs, .combine = "rbind") %do% {
html <- paste0("https://vyhledavac.nssoud.cz/DokumentDetail/Index/", NSS_ID) %>%
read_html()
length_applied_judgments <- length(html_elements(html, xpath = "//td[@title = 'prejudikaturaoznacenivecivcelku']/../../../../table/tbody/tr"))
doc_id <- html_elements(html, xpath ="//*[@class='det-textitle' and contains(@title, 'ECLI (ecli)')]/../*[@class='det-textval']")
if (length(doc_id) == 0) {
doc_id <- NA
} else {doc_id <- html_text(doc_id)}
if(length_applied_judgments == 0) {
return(NULL)
} else {
applied_judgments_result <- foreach(i = seq(length_applied_judgments), .combine = "rbind") %do% {
inner_length <- length(html_elements(html, xpath = "//td[@title = 'prejudikaturaoznacenivecivcelku']/../../../../table/tbody/tr[1]/td"))
output <- tibble(
"doc_id" = doc_id,
"applied_judgment_case_id" = html_elements(html, xpath = paste0("//td[@title = 'prejudikaturaoznacenivecivcelku']/../../../../table/tbody/tr[",i,"]/td[1]")) %>% html_text2(),
"applied_judgment_authority" = html_elements(html, xpath = paste0("//td[@title = 'prejudikaturaoznacenivecivcelku']/../../../../table/tbody/tr[",i,"]/td[", inner_length-2,"]")) %>% html_text2(),
"applied_judgment_relationship" = html_elements(html, xpath = paste0("//td[@title = 'prejudikaturaoznacenivecivcelku']/../../../../table/tbody/tr[",i,"]/td[", inner_length-1,"]")) %>% html_text2(),
"applied_judgment_conflict" = html_elements(html, xpath = paste0("//td[@title = 'prejudikaturaoznacenivecivcelku']/../../../../table/tbody/tr[",i,"]/td[", inner_length, "]")) %>% html_text2()
)
return(output)
}
return(applied_judgments_result)
}
pb$tick()
}
} else {
# Start the parallelezatin process
n.cores <- parallel::detectCores() - 1
my.cluster <- parallel::makeCluster(
n.cores,
type = "PSOCK"
)
#register it to be used by %dopar%
doParallel::registerDoParallel(cl = my.cluster)
#check if it is registered (optional)
foreach::getDoParRegistered()
foreach::getDoParWorkers()
message("Succcesfully finished parallelization")
# rvest loop
NSS_applied_judgments <- foreach(NSS_ID = NSS_IDs, .combine = "rbind", .packages = c("httr", "rvest", "rapportools", "foreach", "tidyverse", "lubridate", "xml2")) %dopar% {
html <- paste0("https://vyhledavac.nssoud.cz/DokumentDetail/Index/", NSS_ID) %>%
read_html()
length_applied_judgments <- length(html_elements(html, xpath = "//td[@title = 'prejudikaturaoznacenivecivcelku']/../../../../table/tbody/tr"))
doc_id <- html_elements(html, xpath ="//*[@class='det-textitle' and contains(@title, 'ECLI (ecli)')]/../*[@class='det-textval']")
if (length(doc_id) == 0) {
doc_id <- NA
} else {doc_id <- html_text(doc_id)}
if(length_applied_judgments == 0) {
return(NULL)
} else {
applied_judgments_result <- foreach(i = seq(length_applied_judgments), .combine = "rbind") %do% {
inner_length <- length(html_elements(html, xpath = "//td[@title = 'prejudikaturaoznacenivecivcelku']/../../../../table/tbody/tr[1]/td"))
output <- tibble(
"doc_id" = doc_id,
"applied_judgment_case_id" = html_elements(html, xpath = paste0("//td[@title = 'prejudikaturaoznacenivecivcelku']/../../../../table/tbody/tr[",i,"]/td[1]")) %>% html_text2(),
"applied_judgment_authority" = html_elements(html, xpath = paste0("//td[@title = 'prejudikaturaoznacenivecivcelku']/../../../../table/tbody/tr[",i,"]/td[", inner_length-2,"]")) %>% html_text2(),
"applied_judgment_relationship" = html_elements(html, xpath = paste0("//td[@title = 'prejudikaturaoznacenivecivcelku']/../../../../table/tbody/tr[",i,"]/td[", inner_length-1,"]")) %>% html_text2(),
"applied_judgment_conflict" = html_elements(html, xpath = paste0("//td[@title = 'prejudikaturaoznacenivecivcelku']/../../../../table/tbody/tr[",i,"]/td[", inner_length, "]")) %>% html_text2()
)
return(output)
}
return(applied_judgments_result)
}
}
# Stop the parallel process
parallel::stopCluster(cl = my.cluster)
}
return(NSS_applied_judgments)
}
NSS_applied_judgments[[7]] <- get_applied_judgments(NSS_IDs = NSS_IDs[60001:69600], par = FALSE)
get_applied_judgments <- function(NSS_IDs, par = FALSE) {
if (par == FALSE) {
pb <- progress_bar$new(
format = "  downloading [:bar] :current/:total :percent eta: :eta",
total = length(NSS_IDs), clear = FALSE, width = 80)
NSS_applied_judgments <- foreach(NSS_ID = NSS_IDs, .combine = "rbind") %do% {
pb$tick()
html <- paste0("https://vyhledavac.nssoud.cz/DokumentDetail/Index/", NSS_ID) %>%
read_html()
length_applied_judgments <- length(html_elements(html, xpath = "//td[@title = 'prejudikaturaoznacenivecivcelku']/../../../../table/tbody/tr"))
doc_id <- html_elements(html, xpath ="//*[@class='det-textitle' and contains(@title, 'ECLI (ecli)')]/../*[@class='det-textval']")
if (length(doc_id) == 0) {
doc_id <- NA
} else {doc_id <- html_text(doc_id)}
if(length_applied_judgments == 0) {
return(NULL)
} else {
applied_judgments_result <- foreach(i = seq(length_applied_judgments), .combine = "rbind") %do% {
inner_length <- length(html_elements(html, xpath = "//td[@title = 'prejudikaturaoznacenivecivcelku']/../../../../table/tbody/tr[1]/td"))
output <- tibble(
"doc_id" = doc_id,
"applied_judgment_case_id" = html_elements(html, xpath = paste0("//td[@title = 'prejudikaturaoznacenivecivcelku']/../../../../table/tbody/tr[",i,"]/td[1]")) %>% html_text2(),
"applied_judgment_authority" = html_elements(html, xpath = paste0("//td[@title = 'prejudikaturaoznacenivecivcelku']/../../../../table/tbody/tr[",i,"]/td[", inner_length-2,"]")) %>% html_text2(),
"applied_judgment_relationship" = html_elements(html, xpath = paste0("//td[@title = 'prejudikaturaoznacenivecivcelku']/../../../../table/tbody/tr[",i,"]/td[", inner_length-1,"]")) %>% html_text2(),
"applied_judgment_conflict" = html_elements(html, xpath = paste0("//td[@title = 'prejudikaturaoznacenivecivcelku']/../../../../table/tbody/tr[",i,"]/td[", inner_length, "]")) %>% html_text2()
)
return(output)
}
return(applied_judgments_result)
}
}
} else {
# Start the parallelezatin process
n.cores <- parallel::detectCores() - 1
my.cluster <- parallel::makeCluster(
n.cores,
type = "PSOCK"
)
#register it to be used by %dopar%
doParallel::registerDoParallel(cl = my.cluster)
#check if it is registered (optional)
foreach::getDoParRegistered()
foreach::getDoParWorkers()
message("Succcesfully finished parallelization")
# rvest loop
NSS_applied_judgments <- foreach(NSS_ID = NSS_IDs, .combine = "rbind", .packages = c("httr", "rvest", "rapportools", "foreach", "tidyverse", "lubridate", "xml2")) %dopar% {
html <- paste0("https://vyhledavac.nssoud.cz/DokumentDetail/Index/", NSS_ID) %>%
read_html()
length_applied_judgments <- length(html_elements(html, xpath = "//td[@title = 'prejudikaturaoznacenivecivcelku']/../../../../table/tbody/tr"))
doc_id <- html_elements(html, xpath ="//*[@class='det-textitle' and contains(@title, 'ECLI (ecli)')]/../*[@class='det-textval']")
if (length(doc_id) == 0) {
doc_id <- NA
} else {doc_id <- html_text(doc_id)}
if(length_applied_judgments == 0) {
return(NULL)
} else {
applied_judgments_result <- foreach(i = seq(length_applied_judgments), .combine = "rbind") %do% {
inner_length <- length(html_elements(html, xpath = "//td[@title = 'prejudikaturaoznacenivecivcelku']/../../../../table/tbody/tr[1]/td"))
output <- tibble(
"doc_id" = doc_id,
"applied_judgment_case_id" = html_elements(html, xpath = paste0("//td[@title = 'prejudikaturaoznacenivecivcelku']/../../../../table/tbody/tr[",i,"]/td[1]")) %>% html_text2(),
"applied_judgment_authority" = html_elements(html, xpath = paste0("//td[@title = 'prejudikaturaoznacenivecivcelku']/../../../../table/tbody/tr[",i,"]/td[", inner_length-2,"]")) %>% html_text2(),
"applied_judgment_relationship" = html_elements(html, xpath = paste0("//td[@title = 'prejudikaturaoznacenivecivcelku']/../../../../table/tbody/tr[",i,"]/td[", inner_length-1,"]")) %>% html_text2(),
"applied_judgment_conflict" = html_elements(html, xpath = paste0("//td[@title = 'prejudikaturaoznacenivecivcelku']/../../../../table/tbody/tr[",i,"]/td[", inner_length, "]")) %>% html_text2()
)
return(output)
}
return(applied_judgments_result)
}
}
# Stop the parallel process
parallel::stopCluster(cl = my.cluster)
}
return(NSS_applied_judgments)
}
NSS_applied_judgments[[7]] <- get_applied_judgments(NSS_IDs = NSS_IDs[60001:69600], par = FALSE)
NSS_applied_judgments[[8]] <- get_applied_judgments(NSS_IDs = NSS_IDs[68000:69600], par = FALSE)
NSS_applied_judgments[[8]] <- get_applied_judgments(NSS_IDs = NSS_IDs[68000:69000], par = FALSE)
View(NSS_applied_judgments)
NSS_applied_judgments_WIP <- NSS_applied_judgments %>% bind_rows() %>% df_unlist() %>% as.tibble()
View(NSS_applied_judgments_WIP)
?rename
yeet <- NSS_applied_judgments
NSS_applied_judgments <- NSS_applied_judgments_WIP
# Save the data
# save(NSS_IDs, file = "data/NSS_IDs.RData")
# save(NSS_metadata, file = "data/NSS_metadata.RData")
# save(NSS_texts, file = "data/NSS_texts.RData")
save(NSS_applied_judgments, file = "data/NSS_applied_judgments.RData")
xfun::pkg_attach2("tidyverse", "tidytext", "ggplot2", "quanteda", "quanteda.textmodels", "ldatuning", "reshape2")
load(file = "models/US_UDmodel.RData")
text_corpus <- data_ud %>% filter(upos %in% c("NOUN", "ADJ")) %>% select(doc_id, paragraph_id, lemma) %>% group_by(doc_id) %>% summarise(text = paste(lemma, collapse = " "))
# Topic modelling
# Stopwords
# Make a list of procedural words to remove
to_remove <- c(
"člán[a-ž]*", "soud[a-ž]*", "odstav[a-ž]*", "rozsud[a-ž]*", "případ[a-ž]*",
"řízení", "odvolání", "stížnost[a-ž]*",
"stěžovatel[a-ž]*", "zákon[a-ž]*", "ústavní[a-ž]*", "návrh[a-ž]*", "odůvodnění[a-ž]*", "náležitost[a-ž]*", "česk[a-ž]*", "navrhovatel[a-ž]*"
)
# Creating a dtm/dfm multiple ways---
# Quanteda
quanteda_corpus <- text_corpus %>% corpus(
docid_field = "doc_id",
text_field = "text",
unique_docnames = FALSE
)
tokens_US <- tokens(quanteda_corpus, remove_punct = TRUE, remove_symbols = TRUE, remove_numbers = TRUE) %>% tokens_select(pattern = to_remove, valuetype = "regex", selection = "remove", min_nchar=2L)
US_dfm <- dfm(tokens_US) %>% dfm_trim(min_termfreq = 5)
result <- ldatuning::FindTopicsNumber(
US_dfm,
topics = seq(from = 10, to = 24, by = 2),
metrics = c("CaoJuan2009",  "Deveaud2014"),
method = "Gibbs",
control = list(seed = 77),
verbose = TRUE
)
FindTopicsNumber_plot(result)
result2 <- ldatuning::FindTopicsNumber(
US_dfm,
topics = seq(from = 23, to = 26, by = 1),
metrics = c("CaoJuan2009",  "Deveaud2014"),
method = "Gibbs",
control = list(seed = 77),
verbose = TRUE
)
result2 <- ldatuning::FindTopicsNumber(
US_dfm,
topics = seq(from = 23, to = 26, by = 1),
metrics = c("CaoJuan2009",  "Deveaud2014"),
method = "Gibbs",
control = list(seed = 77),
verbose = TRUE
)
save.image("~/Library/CloudStorage/OneDrive-Humboldt-UniversitaetzuBerlin,CMS/Programming/apex_courts_dataset/topic_modelling_WIP.RData")
remove(data_ud)
load("~/Library/CloudStorage/OneDrive-Humboldt-UniversitaetzuBerlin,CMS/Programming/apex_courts_dataset/topic_modelling_WIP.RData")
xfun::pkg_attach2("tidyverse", "tidytext", "ggplot2", "quanteda", "quanteda.textmodels", "ldatuning", "reshape2")
result2 <- ldatuning::FindTopicsNumber(
US_dfm,
topics = seq(from = 23, to = 26, by = 1),
metrics = c("CaoJuan2009",  "Deveaud2014"),
method = "Gibbs",
control = list(seed = 77),
verbose = TRUE
)
FindTopicsNumber_plot(result2)
K <- 24
# set random number generator seed
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
US_LDA <- textmodel_lda(US_dfm, K)
xfun::pkg_attach2("tidyverse", "tidytext", "ggplot2", "quanteda", "quanteda.textmodels", "ldatuning", "reshape2")
K <- 24
# set random number generator seed
set.seed(9161)
# compute the LDA model, inference via 1000 iterations of Gibbs sampling
US_LDA <- textmodel_lda(US_dfm, K)
xfun::pkg_attach2("tidyverse", "tidytext", "ggplot2", "quanteda", "quanteda.textmodels", "ldatuning", "reshape2", "seededlda")
US_LDA <- textmodel_lda(US_dfm, K)
save(US_LDA, file = "models/US_LDA.RData")
xfun::pkg_attach2("tidyverse", "tidytext", "ggplot2", "quanteda", "quanteda.textmodels", "ldatuning", "reshape2", "seededlda")
save(US_LDA, file = "models/US_LDA.RData")
terms(US_LDA)
save(US_LDA, file = "models/US_LDA.RData")
default <- NULL
try(default <- read.csv("possibly-bad-input.csv"), silent = TRUE)
try(default <- read.csv("possibly-bad-input.csv"), silent = TRUE)
xfun::pkg_attach2("tidyverse", "tidytext", "ggplot2", "progress", "tm", "foreach", "jsonlite", "rapport")
judgments_annotated <- jsonlite::fromJSON(txt = "data/US_judgments_annotated.json")
View(judgments_annotated)
judgments_annotations <- judgments_annotated$examples %>% as.data.frame()
View(judgments_annotations)
View(judgments_annotations[[6]][[1]])
df <- foreach(i = seq(judgments_annotations[[6]]), .combine = "rbind") %:%
foreach (j = seq(judgments_annotations[[6]][[i]]), .combine = "rbind") %do% {
output <- list(
"doc_id" = judgments_annotations$metadata$doc_id[i],
"value" = judgments_annotations[[6]][[i]][j,4],
"tag" = judgments_annotations[[6]][[i]][j,2],
"start" = judgments_annotations[[6]][[i]][j,3],
"end" = judgments_annotations[[6]][[i]][j,1]
)
return(output)
} %>% as.data.frame(row.names = FALSE)
df <- foreach(i = seq(judgments_annotations[[6]]), .combine = "rbind") %:%
foreach (j = seq(judgments_annotations[[6]][[i]]), .combine = "rbind") %do% {
output <- list(
"doc_id" = judgments_annotations$metadata$doc_id[i],
"value" = judgments_annotations[[6]][[i]][j,4],
"tag" = judgments_annotations[[6]][[i]][j,2],
"start" = judgments_annotations[[6]][[i]][j,3],
"end" = judgments_annotations[[6]][[i]][j,1]
)
return(output)
} %>% as.data.frame(row.names = FALSE)
judgments_annotations <- judgments_annotated$examples %>% as.data.frame()
View(df)
#Load data
source("supporting_functions.R")
df <- foreach(i = seq(judgments_annotations[[6]]), .combine = "rbind") %:%
foreach (j = seq(judgments_annotations[[6]][[i]]), .combine = "rbind") %do% {
output <- list(
"doc_id" = judgments_annotations$metadata$doc_id[i],
"value" = judgments_annotations[[6]][[i]][j,4],
"tag" = judgments_annotations[[6]][[i]][j,2],
"start" = judgments_annotations[[6]][[i]][j,3],
"end" = judgments_annotations[[6]][[i]][j,1]
)
return(output)
} %>% as.data.frame(row.names = FALSE) %>% df_unlist()
View(output)
View(df)
df <- foreach(i = seq(judgments_annotations[[6]]), .combine = "rbind") %:%
foreach (j = seq(judgments_annotations[[6]][[i]]), .combine = "rbind") %do% {
output <- list(
"doc_id" = judgments_annotations$metadata$doc_id[i],
"value" = judgments_annotations[[6]][[i]][j,4],
"tag" = judgments_annotations[[6]][[i]][j,2],
"start" = judgments_annotations[[6]][[i]][j,3],
"end" = judgments_annotations[[6]][[i]][j,1]
)
return(output)
} %>% as.data.frame(row.names = FALSE) %>% df_unlist() %>% drop_na()
View(df)
df <- foreach(i = seq(judgments_annotations[[6]]), .combine = "rbind") %:%
foreach (j = seq(judgments_annotations[[6]][[i]]), .combine = "rbind") %do% {
output <- list(
"doc_id" = judgments_annotations$metadata$doc_id[i],
"value" = judgments_annotations[[6]][[i]][j,4],
"tag" = judgments_annotations[[6]][[i]][j,2],
"start" = judgments_annotations[[6]][[i]][j,3],
"end" = judgments_annotations[[6]][[i]][j,1]
)
return(output)
} %>% as.data.frame(row.names = FALSE) %>% df_unlist() %>% drop_na() %>% as.tibble()
df <- foreach(i = seq(judgments_annotations[[6]]), .combine = "rbind") %:%
foreach (j = seq(judgments_annotations[[6]][[i]]), .combine = "rbind") %do% {
output <- list(
"doc_id" = judgments_annotations$metadata$doc_id[i],
"value" = judgments_annotations[[6]][[i]][j,4],
"tag" = judgments_annotations[[6]][[i]][j,2],
"start" = judgments_annotations[[6]][[i]][j,3],
"end" = judgments_annotations[[6]][[i]][j,1]
)
return(output)
} %>% as.data.frame(row.names = FALSE) %>% df_unlist() %>% drop_na() %>% as_tibble()
install.packages(c("BH", "bit", "blob", "boot", "broom", "bslib", "cachem", "campfin", "car", "class", "cli", "codetools", "collapse", "colorspace", "commonmark", "curl", "data.table", "dbplyr", "digest", "dplyr", "dtplyr", "evaluate", "fansi", "fastmap", "fixest", "fontawesome", "forcats", "foreign", "Formula", "fs", "gapminder", "gargle", "ggplot2", "ggpubr", "ggrepel", "ggsci", "gh", "glmnet", "googledrive", "googlesheets4", "gtable", "highr", "hms", "htmltools", "htmlwidgets", "httpuv", "httr", "igraph", "isoband", "janitor", "jsonlite", "knitr", "lattice", "lme4", "lpSolve", "lubridate", "markdown", "MASS", "Matrix", "mgcv", "modelr", "nlme", "openssl", "pbapply", "pbkrtest", "pdftools", "pillar", "plm", "ps", "purrr", "qpdf", "quanteda", "quanteda.textmodels", "quantreg", "Rcpp", "RcppArmadillo", "RcppParallel", "readr", "readxl", "rlang", "rmarkdown", "rstatix", "sass", "sourcetools", "spatial", "stringi", "stringr", "survival", "testthat", "tibble", "tidyr", "tidyverse", "timechange", "tinytex", "tm", "udpipe", "utf8", "vctrs", "vroom", "xfun", "yaml", "zoo"))
install.packages(c("BH", "bit", "blob", "boot", "broom", "bslib", "cachem", "campfin", "car", "class", "cli", "codetools", "collapse", "colorspace", "commonmark", "curl", "data.table", "dbplyr", "digest", "dplyr", "dtplyr", "evaluate", "fansi", "fastmap", "fixest", "fontawesome", "forcats", "foreign", "Formula", "fs", "gapminder", "gargle", "ggplot2", "ggpubr", "ggrepel", "ggsci", "gh", "glmnet", "googledrive", "googlesheets4", "gtable", "highr", "hms", "htmltools", "htmlwidgets", "httpuv", "httr", "igraph", "isoband", "janitor", "jsonlite", "knitr", "lattice", "lme4", "lpSolve", "lubridate", "markdown", "MASS", "Matrix", "mgcv", "modelr", "nlme", "openssl", "pbapply", "pbkrtest", "pdftools", "pillar", "plm", "ps", "purrr", "qpdf", "quanteda", "quanteda.textmodels", "quantreg", "Rcpp", "RcppArmadillo", "RcppParallel", "readr", "readxl", "rlang", "rmarkdown", "rstatix", "sass", "sourcetools", "spatial", "stringi", "stringr", "survival", "testthat", "tibble", "tidyr", "tidyverse", "timechange", "tinytex", "tm", "udpipe", "utf8", "vctrs", "vroom", "xfun", "yaml", "zoo"))
install.packages(c("BH", "bit", "blob", "boot", "broom", "bslib", "cachem", "campfin", "car", "class", "cli", "codetools", "collapse", "colorspace", "commonmark", "curl", "data.table", "dbplyr", "digest", "dplyr", "dtplyr", "evaluate", "fansi", "fastmap", "fixest", "fontawesome", "forcats", "foreign", "Formula", "fs", "gapminder", "gargle", "ggplot2", "ggpubr", "ggrepel", "ggsci", "gh", "glmnet", "googledrive", "googlesheets4", "gtable", "highr", "hms", "htmltools", "htmlwidgets", "httpuv", "httr", "igraph", "isoband", "janitor", "jsonlite", "knitr", "lattice", "lme4", "lpSolve", "lubridate", "markdown", "MASS", "Matrix", "mgcv", "modelr", "nlme", "openssl", "pbapply", "pbkrtest", "pdftools", "pillar", "plm", "ps", "purrr", "qpdf", "quanteda", "quanteda.textmodels", "quantreg", "Rcpp", "RcppArmadillo", "RcppParallel", "readr", "readxl", "rlang", "rmarkdown", "rstatix", "sass", "sourcetools", "spatial", "stringi", "stringr", "survival", "testthat", "tibble", "tidyr", "tidyverse", "timechange", "tinytex", "tm", "udpipe", "utf8", "vctrs", "vroom", "xfun", "yaml", "zoo"))
install.packages(c("BH", "bit", "blob", "boot", "broom", "bslib", "cachem", "campfin", "car", "class", "cli", "codetools", "collapse", "colorspace", "commonmark", "curl", "data.table", "dbplyr", "digest", "dplyr", "dtplyr", "evaluate", "fansi", "fastmap", "fixest", "fontawesome", "forcats", "foreign", "Formula", "fs", "gapminder", "gargle", "ggplot2", "ggpubr", "ggrepel", "ggsci", "gh", "glmnet", "googledrive", "googlesheets4", "gtable", "highr", "hms", "htmltools", "htmlwidgets", "httpuv", "httr", "igraph", "isoband", "janitor", "jsonlite", "knitr", "lattice", "lme4", "lpSolve", "lubridate", "markdown", "MASS", "Matrix", "mgcv", "modelr", "nlme", "openssl", "pbapply", "pbkrtest", "pdftools", "pillar", "plm", "ps", "purrr", "qpdf", "quanteda", "quanteda.textmodels", "quantreg", "Rcpp", "RcppArmadillo", "RcppParallel", "readr", "readxl", "rlang", "rmarkdown", "rstatix", "sass", "sourcetools", "spatial", "stringi", "stringr", "survival", "testthat", "tibble", "tidyr", "tidyverse", "timechange", "tinytex", "tm", "udpipe", "utf8", "vctrs", "vroom", "xfun", "yaml", "zoo"))
?doc2vec
xfun::pkg_attach2("tidyverse", "tidytext", "ggplot2", "progress", "tm", "foreach", "jsonlite",  "word2vec")
View(df)
View(df)
load(file = "models/word2vec_model_CBOW.RData")
doc2vec(word2vec_model_CBOW, df)
View(word2vec_model_skipgram)
doc2vec(word2vec_model_skipgram, df)
doc2vec(word2vec_model_skipgram$model, df)
df <- foreach(i = seq(judgments_annotations[[6]]), .combine = "rbind") %:%
foreach (j = seq(judgments_annotations[[6]][[i]]), .combine = "rbind") %do% {
output <- list(
"doc_id" = judgments_annotations$metadata$doc_id[i],
"value" = judgments_annotations[[6]][[i]][j,4],
"tag" = judgments_annotations[[6]][[i]][j,2],
"start" = judgments_annotations[[6]][[i]][j,3],
"end" = judgments_annotations[[6]][[i]][j,1]
)
return(output)
} %>% as.data.frame(row.names = FALSE) %>% df_unlist() %>% drop_na() %>% as_tibble() %>% select(doc_id, value)
View(df)
doc2vec(word2vec_model_skipgram, df)
doc2vec(word2vec_model_skipgram, x = df)
df <- foreach(i = seq(judgments_annotations[[6]]), .combine = "rbind") %:%
foreach (j = seq(judgments_annotations[[6]][[i]]), .combine = "rbind") %do% {
output <- list(
"doc_id" = judgments_annotations$metadata$doc_id[i],
"value" = judgments_annotations[[6]][[i]][j,4],
"tag" = judgments_annotations[[6]][[i]][j,2],
"start" = judgments_annotations[[6]][[i]][j,3],
"end" = judgments_annotations[[6]][[i]][j,1]
)
return(output)
} %>% as.data.frame(row.names = FALSE) %>% df_unlist() %>% drop_na() %>% as_tibble() %>% select(doc_id, value) %>% rename(text = value)
View(df)
doc2vec(word2vec_model_skipgram, x = df)
doc2vec(word2vec_model_skipgram, x = df, type = "embedding")
?doc2vec
doc2vec(word2vec_model_skipgram, newdata = df, type = "embedding")
load(file = "models/word2vec_model_CBOW.RData")
read.word2vec(file = "models/word2vec_model_CBOW.RData")
load("data/US_texts.RData")
# Word2vec
# Window parameter:  for skip-gram usually around 10, for cbow around 5
# Sample: sub-sampling of frequent words: can improve both accuracy and
# speed for large data sets (useful values are in range 0.001 to 0.00001)
# # hs: the training algorithm: hierarchical so􏰂max (better for infrequent
# words) vs nega􏰁ve sampling (better for frequent words, better with low
#                              dimensional vectors)
word2vec_model_CBOW <- word2vec(x = data_texts$text, dim = 300)
# Word2vec
# Window parameter:  for skip-gram usually around 10, for cbow around 5
# Sample: sub-sampling of frequent words: can improve both accuracy and
# speed for large data sets (useful values are in range 0.001 to 0.00001)
# # hs: the training algorithm: hierarchical so􏰂max (better for infrequent
# words) vs nega􏰁ve sampling (better for frequent words, better with low
#                              dimensional vectors)
word2vec_model_CBOW <- word2vec(x = data_texts$text, dim = 300)
# Word2vec
# Window parameter:  for skip-gram usually around 10, for cbow around 5
# Sample: sub-sampling of frequent words: can improve both accuracy and
# speed for large data sets (useful values are in range 0.001 to 0.00001)
# # hs: the training algorithm: hierarchical so􏰂max (better for infrequent
# words) vs nega􏰁ve sampling (better for frequent words, better with low
#                              dimensional vectors)
word2vec_model_CBOW <- word2vec(x = US_texts$text, dim = 300)
doc2vec(word2vec_model_CBOW, newdata = df, type = "embedding")
doc2vec_model <- doc2vec(word2vec_model_CBOW, newdata = df, type = "embedding")
View(doc2vec_model)
View(doc2vec_model)
doc2vec_model <- doc2vec(word2vec_model_CBOW, newdata = df, type = "embedding") %>% rownames_to_column(var = "doc_id")
doc2vec_model <- doc2vec(word2vec_model_CBOW, newdata = df, type = "embedding") as_tibble() %>% rownames_to_column(var = "doc_id")
doc2vec_model <- doc2vec(word2vec_model_CBOW, newdata = df, type = "embedding") %>% as_tibble() %>% rownames_to_column(var = "doc_id")
doc2vec_model <- doc2vec(word2vec_model_CBOW, newdata = df, type = "embedding") %>% as_tibble()
View(doc2vec_model)
doc2vec_model <- doc2vec(word2vec_model_CBOW, newdata = df, type = "embedding") %>% as.data.frame() %>% rownames_to_column(var = "doc_id")
View(doc2vec_model)
View(df)
?make.unique
df <- foreach(i = seq(judgments_annotations[[6]]), .combine = "rbind") %:%
foreach (j = seq(judgments_annotations[[6]][[i]]), .combine = "rbind") %do% {
output <- list(
"doc_id" = judgments_annotations$metadata$doc_id[i],
"value" = judgments_annotations[[6]][[i]][j,4],
"tag" = judgments_annotations[[6]][[i]][j,2],
"start" = judgments_annotations[[6]][[i]][j,3],
"end" = judgments_annotations[[6]][[i]][j,1]
)
return(output)
} %>% as.data.frame(row.names = FALSE) %>% df_unlist() %>% drop_na() %>% as_tibble()
doc2vec_model <- doc2vec(word2vec_model_CBOW, newdata = df, type = "embedding") %>% as.data.frame() %>% rownames_to_column(var = "doc_id")
doc2vec_model <- doc2vec(word2vec_model_CBOW, newdata = df, type = "embedding") %>% as.data.frame() %>% rownames_to_column(var = "doc_id")
doc2vec_model <- doc2vec(word2vec_model_CBOW, newdata = df, type = "embedding") %>% as.data.frame() %>% rownames_to_column(var = "doc_id")
doc2vec_model <- doc2vec(word2vec_model_CBOW, newdata = df, type = "embedding")
View(doc2vec_model)
df$doc_id <- df$doc_id %>% make.unique()
View(df)
df_doc <- df %>% select(doc_id, value) %>% rename(text = value)
doc2vec_model <- doc2vec(word2vec_model_CBOW, newdata = df_doc, type = "embedding") %>% as.data.frame() %>% rownames_to_column(var = "doc_id")
View(doc2vec_model)
View(df)
doc2vec_model <- df %>% select(-value)
doc2vec_model <- doc2vec(word2vec_model_CBOW, newdata = df_doc, type = "embedding") %>% as.data.frame() %>% rownames_to_column(var = "doc_id")
dat <- df %>% select(-value) %>% left_join(doc2vec_model, .)
View(dat)
# SVM learning
# Fit Support Vector Machine model to data set with e1071
svmfit <- svm(tag~., data = dat, kernel = "linear", scale = FALSE)
dat$tag <- dat$tag %>% as.factor()
xfun::pkg_attach2("tidyverse", "ggplot2", "progress", "foreach", "jsonlite",  "word2vec", "svm")
?svm
xfun::pkg_attach2("tidyverse", "ggplot2", "progress", "foreach", "jsonlite",  "word2vec", "e1071")
# SVM learning
# Fit Support Vector Machine model to data set with e1071
svmfit <- svm(tag~., data = dat, kernel = "linear", scale = FALSE)
dat <- df %>% select(-value) %>% left_join(doc2vec_model, .) %>% column_to_rownames(var ="doc_id")
View(dat)
dat$tag <- dat$tag %>% as.factor()
# SVM learning
# Fit Support Vector Machine model to data set with e1071
svmfit <- svm(tag~., data = dat, kernel = "linear", scale = FALSE)
# Plot Results
plot(svmfit, dat)
# Plot Results
plot(svmfit, dat)
print(svmfit)
tune.out <- tune(svm, y~., data = dat, kernel = "linear",
ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
tune.out <- tune(svm, tag~., data = dat, kernel = "linear",
ranges = list(cost = c(0.001, 0.01, 0.1, 1, 5, 10, 100)))
# extract the best model
(bestmod <- tune.out$best.model)
?svm
# SVM learning
# Fit Support Vector Machine model to data set with e1071
svmfit <- svm(tag~., data = dat, kernel = "linear", scale = FALSE, cost = 0.01)
ypred <- predict(bestmod, dat)
(misclass <- table(predict = ypred, truth = dat$y))
(misclass <- table(predict = svmfit, truth = dat$y))
(misclass <- table(predict = svmfit, truth = dat$tag))
# Create a table of misclassified observations
ypred <- predict(svmfit, dat)
(misclass <- table(predict = ypred, truth = dat$tag))
