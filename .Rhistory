#Save the whole judgment text
text_temp <- judgments_annotations$content[judgments_annotations$metadata$doc_id == judgments_annotations_parts$doc_id[i]]
# Split the original parts tibble into paragraph level observations
output <- foreach(j = seq(judgments_annotations_paragraphs_temp), .combine = "rbind") %do% {
location_temp <- str_locate(string = text_temp, pattern = fixed(judgments_annotations_paragraphs_temp[j])) %>% as.list()
text_length <- str_length(text_temp)
output <- tibble(
"doc_id" = judgments_annotations_parts$doc_id[i],
"paragraph_id" =
"value" = judgments_annotations_paragraphs_temp[j],
judgments_annotations_paragraphs <- foreach(i = seq(nrow(judgments_annotations_parts)), .combine = "rbind") %do% {
# Create the temporary vector of paragraph texts
judgments_annotations_paragraphs_temp <- judgments_annotations_parts$value[i] %>% str_split(pattern = "\n\n")
judgments_annotations_paragraphs_temp <- judgments_annotations_paragraphs_temp[[1]] %>% as.vector()
judgments_annotations_paragraphs_temp <- judgments_annotations_paragraphs_temp[! judgments_annotations_paragraphs_temp %in% c("")]
#Save the whole judgment text
text_temp <- judgments_annotations$content[judgments_annotations$metadata$doc_id == judgments_annotations_parts$doc_id[i]]
# Split the original parts tibble into paragraph level observations
output <- foreach(j = seq(judgments_annotations_paragraphs_temp), .combine = "rbind") %do% {
location_temp <- str_locate(string = text_temp, pattern = fixed(judgments_annotations_paragraphs_temp[j])) %>% as.list()
text_length <- str_length(text_temp)
output <- tibble(
"doc_id" = judgments_annotations_parts$doc_id[i],
"paragraph_id" =
"value" = judgments_annotations_paragraphs_temp[j],
judgments_annotations_paragraphs <- foreach(i = seq(nrow(judgments_annotations_parts)), .combine = "rbind") %do% {
# Create the temporary vector of paragraph texts
judgments_annotations_paragraphs_temp <- judgments_annotations_parts$value[i] %>% str_split(pattern = "\n\n")
judgments_annotations_paragraphs_temp <- judgments_annotations_paragraphs_temp[[1]] %>% as.vector()
judgments_annotations_paragraphs_temp <- judgments_annotations_paragraphs_temp[! judgments_annotations_paragraphs_temp %in% c("")]
#Save the whole judgment text
text_temp <- judgments_annotations$content[judgments_annotations$metadata$doc_id == judgments_annotations_parts$doc_id[i]]
# Split the original parts tibble into paragraph level observations
output <- foreach(j = seq(judgments_annotations_paragraphs_temp), .combine = "rbind") %do% {
location_temp <- str_locate(string = text_temp, pattern = fixed(judgments_annotations_paragraphs_temp[j])) %>% as.list()
text_length <- str_length(text_temp)
output <- tibble(
"doc_id" = judgments_annotations_parts$doc_id[i],
"paragraph_id" =
"value" = judgments_annotations_paragraphs_temp[j],
judgments_annotations_paragraphs <- foreach(i = seq(nrow(judgments_annotations_parts)), .combine = "rbind") %do% {
# Create the temporary vector of paragraph texts
judgments_annotations_paragraphs_temp <- judgments_annotations_parts$value[i] %>% str_split(pattern = "\n\n")
judgments_annotations_paragraphs_temp <- judgments_annotations_paragraphs_temp[[1]] %>% as.vector()
judgments_annotations_paragraphs_temp <- judgments_annotations_paragraphs_temp[! judgments_annotations_paragraphs_temp %in% c("")]
#Save the whole judgment text
text_temp <- judgments_annotations$content[judgments_annotations$metadata$doc_id == judgments_annotations_parts$doc_id[i]]
# Split the original parts tibble into paragraph level observations
output <- foreach(j = seq(judgments_annotations_paragraphs_temp), .combine = "rbind") %do% {
location_temp <- str_locate(string = text_temp, pattern = fixed(judgments_annotations_paragraphs_temp[j])) %>% as.list()
text_length <- str_length(text_temp)
output <- tibble(
"doc_id" = judgments_annotations_parts$doc_id[i],
"value" = judgments_annotations_paragraphs_temp[j],
"tag" = judgments_annotations_parts$tag[i],
"start" = as.numeric(location_temp[1])/text_length,
"end" = as.numeric(location_temp[2])/text_length,
"length" = str_length(judgments_annotations_paragraphs_temp[j])/text_length
)
return(output)
}
}
# Create paragraph IDs within group
judgments_annotations_paragraphs <- judgments_annotations_paragraphs %>%
group_by(doc_id) %>%
mutate(paragraph_id = as.integer(gl(n(), 2, n()))) %>%
ungroup()
View(judgments_annotations_paragraphs)
judgments_annotations_paragraphs <- foreach(i = seq(nrow(judgments_annotations_parts)), .combine = "rbind") %do% {
# Create the temporary vector of paragraph texts
judgments_annotations_paragraphs_temp <- judgments_annotations_parts$value[i] %>% str_split(pattern = "\n\n")
judgments_annotations_paragraphs_temp <- judgments_annotations_paragraphs_temp[[1]] %>% as.vector()
judgments_annotations_paragraphs_temp <- judgments_annotations_paragraphs_temp[! judgments_annotations_paragraphs_temp %in% c("")]
#Save the whole judgment text
text_temp <- judgments_annotations$content[judgments_annotations$metadata$doc_id == judgments_annotations_parts$doc_id[i]]
# Split the original parts tibble into paragraph level observations
output <- foreach(j = seq(judgments_annotations_paragraphs_temp), .combine = "rbind") %do% {
location_temp <- str_locate(string = text_temp, pattern = fixed(judgments_annotations_paragraphs_temp[j])) %>% as.list()
text_length <- str_length(text_temp)
output <- tibble(
"doc_id" = judgments_annotations_parts$doc_id[i],
"value" = judgments_annotations_paragraphs_temp[j],
"tag" = judgments_annotations_parts$tag[i],
"start" = as.numeric(location_temp[1])/text_length,
"end" = as.numeric(location_temp[2])/text_length,
"length" = str_length(judgments_annotations_paragraphs_temp[j])/text_length
)
return(output)
}
}
# Create paragraph IDs within group
judgments_annotations_paragraphs <- judgments_annotations_paragraphs %>%
group_by(doc_id) %>%
mutate(paragraph_id = as.integer(gl(n(), 1, n()))) %>%
ungroup()
View(judgments_annotations_paragraphs)
View(judgments_annotations_paragraphs)
View(judgments_annotations_parts)
View(judgments_annotations_paragraphs)
?make.unique
?doc2vec
View(judgments_annotations_paragraphs)
judgments_annotations_paragraphs$doc_id <- judgments_annotations_paragraphs$doc_id %>% make.unique()
View(judgments_annotations_paragraphs)
judgments_annotations_paragraphs <- foreach(i = seq(nrow(judgments_annotations_parts)), .combine = "rbind") %do% {
# Create the temporary vector of paragraph texts
judgments_annotations_paragraphs_temp <- judgments_annotations_parts$value[i] %>% str_split(pattern = "\n\n")
judgments_annotations_paragraphs_temp <- judgments_annotations_paragraphs_temp[[1]] %>% as.vector()
judgments_annotations_paragraphs_temp <- judgments_annotations_paragraphs_temp[! judgments_annotations_paragraphs_temp %in% c("")]
#Save the whole judgment text
text_temp <- judgments_annotations$content[judgments_annotations$metadata$doc_id == judgments_annotations_parts$doc_id[i]]
# Split the original parts tibble into paragraph level observations
output <- foreach(j = seq(judgments_annotations_paragraphs_temp), .combine = "rbind") %do% {
location_temp <- str_locate(string = text_temp, pattern = fixed(judgments_annotations_paragraphs_temp[j])) %>% as.list()
text_length <- str_length(text_temp)
output <- tibble(
"doc_id" = judgments_annotations_parts$doc_id[i],
"value" = judgments_annotations_paragraphs_temp[j],
"tag" = judgments_annotations_parts$tag[i],
"start" = as.numeric(location_temp[1])/text_length,
"end" = as.numeric(location_temp[2])/text_length,
"length" = str_length(judgments_annotations_paragraphs_temp[j])/text_length
)
return(output)
}
} %>%
group_by(doc_id) %>%
mutate(paragraph_id = as.integer(gl(n(), 1, n()))) %>%
ungroup()
View(judgments_annotations_paragraphs)
load("data/doc2vec_df.RData")
View(doc2vec_df)
?modify
?str_remove
doc2vec_df$doc_id <- modify(doc2vec_df$doc_id, .f = str_remove, pattern = "\.\\d+")
doc2vec_df$doc_id <- modify(doc2vec_df$doc_id, .f = str_remove, pattern = ".\\d+")
View(doc2vec_df)
doc2vec_df$doc_id <- modify(doc2vec_df$doc_id, .f = str_remove, pattern = ".\d+")
doc2vec_df$doc_id <- modify(doc2vec_df$doc_id, .f = str_remove, pattern = "\\.\\d+")
View(judgments_annotations_paragraphs)
View(doc2vec_df)
?ifelse
typeof(doc2vec_df$doc_id)
load("data/US_metadata.RData")
View(US_metadata)
US_metadata$dissenting_opinion[[2]]
?modify_if
for(i in seq_along(doc2vec_df$doc_id)){
doc2vec_df$dissent[[i]] <- ifelse(test = US_metadata$dissenting_opinion[[US_metadata$doc_id == doc2vec_df$doc_id[[i]]]] == "", yes = 0, no = 1)
}
for(i in seq_along(doc2vec_df$doc_id)){
doc2vec_df$dissent[[i]] <- ifelse(test = US_metadata$dissenting_opinion[US_metadata$doc_id == doc2vec_df$doc_id[i]] == "", yes = 0, no = 1)
}
View(judgments_annotations_paragraphs)
View(doc2vec_df)
for(i in seq_along(doc2vec_df$doc_id)){
doc2vec_df$dissent[i] <- ifelse(test = US_metadata$dissenting_opinion[US_metadata$doc_id == doc2vec_df$doc_id[i]] == "", yes = 0, no = 1)
}
for(i in seq_along(doc2vec_df$doc_id)){
doc2vec_df$dissent[i] <- ifelse(test = US_metadata$dissenting_opinion[US_metadata$doc_id == doc2vec_df$doc_id[i]] == "", yes = 0, no = 1)
}
for(i in seq_along(doc2vec_df$doc_id)){
doc2vec_df$dissent[i] <- ifelse(test = US_metadata$dissenting_opinion[US_metadata$doc_id == doc2vec_df$doc_id[i]] == "", yes = 0, no = 1)
}
?ifelse
for(i in seq_along(doc2vec_df$doc_id)){
doc2vec_df$dissent[[i]] <- ifelse(test = US_metadata$dissenting_opinion[US_metadata$doc_id == doc2vec_df$doc_id[i]] == "", yes = 0, no = 1)
}
View(doc2vec_df)
?is_empty
doc2vec_df$dissent[[i]] <- ifelse(US_metadata$dissenting_opinion[US_metadata$doc_id == doc2vec_df$doc_id[i]] == is_empty(), 0, 1)
for(i in seq_along(doc2vec_df$doc_id)){
doc2vec_df$dissent[[i]] <- ifelse(US_metadata$dissenting_opinion[US_metadata$doc_id == doc2vec_df$doc_id[i]] == is_empty(), 0, 1)
}
?is.empty
?case_when
doc2vec_df$dissent <- US_metadata %>% select(doc_id, dissenting_opinion) %>% left_join(doc2vec_df, .)
View(doc2vec_df)
US_metadata %>% select(doc_id, dissenting_opinion)
load(file = "data/doc2vec_df.RData")
save(data = doc2vec_df, file = "data/doc2vec_df.RData")
doc2vec_df$doc_id <- modify(doc2vec_df$doc_id, .f = str_remove, pattern = "\\.\\d+")
View(doc2vec_df)
save(data = doc2vec_df, file = "data/doc2vec_df.RData")
View(doc2vec_df)
doc2vec_df$dissent <- US_metadata %>% select(doc_id, dissenting_opinion) %>% left_join(doc2vec_df,.)
View(doc2vec_df)
load(file = "data/doc2vec_df.RData")
View(doc2vec_df)
View(doc2vec_df)
doc2vec_df$dissent <- US_metadata %>% select(doc_id, dissenting_opinion) %>% left_join(.,doc2vec_df)
if_else()
?if_else
doc2vec_df %>% mutate(dissent = US_metadata$dissenting_opionion[US_metadata$doc_id == doc_id] == "", 0, 1)
US_metadata %>% select(doc_id, dissenting_opinion)
doc2vec_df %>% mutate(dissent = dissent == "", 0, 1)
x <- US_metadata %>% select(doc_id, dissenting_opinion) %>% left_join(doc2vec_df, ., by = doc_id)
x <- US_metadata %>% select(doc_id, dissenting_opinion) %>% left_join(doc2vec_df, ., by = "doc_id")
View(x)
doc2vec_df %>% mutate(dissenting_opinion = if_else(dissenting_opinion == "", 0, 1))
doc2vec_df <- US_metadata %>% select(doc_id, dissenting_opinion) %>% left_join(doc2vec_df, ., by = "doc_id")
View(doc2vec_df)
doc2vec_df %>% mutate(dissenting_opinion = ifelse(dissenting_opinion == "", 0, 1))
load(file = "data/doc2vec_df.RData")
doc2vec_df <- US_metadata %>%
select(doc_id, dissenting_opinion) %>%
left_join(doc2vec_df, ., by = "doc_id") %>%
mutate(dissenting_opinion = ifelse(dissenting_opinion == "", 0, 1))
View(doc2vec_df)
save(data = doc2vec_df, file = "data/doc2vec_df.RData")
xfun::pkg_attach2("tidyverse", "ggplot2", "tidymodels")
doc2vec_df <- doc2vec_df %>% modify(.f = unlist) %>% as_tibble()
# Fix the random numbers by setting the seed
# This enables the analysis to be reproducible when random numbers are used
set.seed(222)
# Put 3/4 of the data into the training set
data_split <- initial_split(doc2vec_df, prop = 3/4)
# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)
doc2vec_rec <- recipe(tag ~ ., data = train_data) %>%
update_role(all_double(), new_role = "predictor") %>%
update_role(doc_id, new_role = "ID") %>%
update_role(tag, new_role = "outcome")
doc2vec_rec
svm_mod <- svm_linear(cost = 0.01) %>%
set_engine("kernlab") %>%
set_mode("classification")
svm_mod
doc2vec_wflow <- workflow() %>%
add_model(svm_mod) %>%
add_recipe(doc2vec_rec)
doc2vec_fit <- doc2vec_wflow %>%
fit(data = train_data)
# Measuring the accuracy of the basic model
doc2vec_aug <-
augment(doc2vec_fit, test_data) %>%
select(doc_id, tag, .pred_class) %>%
mutate(
tag = factor(tag),
.pred_class = factor(.pred_class)
)
doc2vec_aug %>%
accuracy(truth = tag, .pred_class)
doc2vec_aug %>%
conf_mat(truth = tag, .pred_class)
doc2vec_rec
xfun::pkg_attach2("tidyverse", "ggplot2", "tidymodels")
load("data/doc2vec_df.RData")
View(doc2vec_df)
doc2vec_df
doc2vec_df <- doc2vec_df %>% modify(.f = unlist) %>% as_tibble()
save(data = doc2vec_df, file = "data/doc2vec_df.RData")
doc2vec_df
doc2vec_df <- doc2vec_df %>% filter(dissenting_opinion == 1) %>% select(!dissenting_opinion)
load("data/doc2vec_df.RData")
View(doc2vec_df)
doc2vec_df <- doc2vec_df %>% filter(dissenting_opinion == 1) %>% select(!dissenting_opinion)
# Fix the random numbers by setting the seed
# This enables the analysis to be reproducible when random numbers are used
set.seed(222)
# Put 3/4 of the data into the training set
data_split <- initial_split(doc2vec_df, prop = 3/4)
# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)
# This creates a recipe object. A recipe object contains the formula for the model as well as additional information for example on the role of the columns in a dataframe (ID, predictor, outcome)
doc2vec_rec <- recipe(tag ~ ., data = train_data) %>%
update_role(all_double(), new_role = "predictor") %>%
update_role(doc_id, new_role = "ID") %>%
update_role(tag, new_role = "outcome")
doc2vec_rec
# This creates a model object, in which you set the model specifications including the parameters to be tuned or the engine of the model
svm_mod <- svm_linear(cost = 0.01) %>%
set_engine("kernlab") %>%
set_mode("classification")
svm_mod
# Bind the model and the recipe to a workflow
doc2vec_wflow <- workflow() %>%
add_model(svm_mod) %>%
add_recipe(doc2vec_rec)
# Instead of training on the whole train data, let's do cross validation
set.seed(345)
folds <- vfold_cv(train_data, v = 6)
doc2vec_fit_cv <- doc2vec_wflow %>%
fit_resamples(folds)
collect_metrics(doc2vec_fit_cv)
doc2vec_df <- doc2vec_df %>% filter(dissenting_opinion == 0) %>% select(!dissenting_opinion)
load("data/doc2vec_df.RData")
doc2vec_df <- doc2vec_df %>% filter(dissenting_opinion == 0) %>% select(!dissenting_opinion)
# Put 3/4 of the data into the training set
data_split <- initial_split(doc2vec_df, prop = 3/4)
# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)
# This creates a recipe object. A recipe object contains the formula for the model as well as additional information for example on the role of the columns in a dataframe (ID, predictor, outcome)
doc2vec_rec <- recipe(tag ~ ., data = train_data) %>%
update_role(all_double(), new_role = "predictor") %>%
update_role(doc_id, new_role = "ID") %>%
update_role(tag, new_role = "outcome")
doc2vec_rec
# This creates a model object, in which you set the model specifications including the parameters to be tuned or the engine of the model
svm_mod <- svm_linear(cost = 0.01) %>%
set_engine("kernlab") %>%
set_mode("classification")
svm_mod
# Bind the model and the recipe to a workflow
doc2vec_wflow <- workflow() %>%
add_model(svm_mod) %>%
add_recipe(doc2vec_rec)
set.seed(345)
folds <- vfold_cv(train_data, v = 6)
# This does a basic cross validation, yet without parameter tuning
doc2vec_fit_cv <- doc2vec_wflow %>%
fit_resamples(folds)
collect_metrics(doc2vec_fit_cv)
load("data/doc2vec_df.RData")
# Put 3/4 of the data into the training set
data_split <- initial_split(doc2vec_df, prop = 3/4)
# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)
source("~/Library/CloudStorage/OneDrive-Humboldt-UniversitaetzuBerlin,CMS/Programming/apex_courts_dataset/judgment_partitioning_tidymodeling.R", echo=TRUE)
# Instead of training on the whole train data, let's do cross validation
set.seed(345)
folds <- vfold_cv(train_data, v = 6)
# This does a basic cross validation, yet without parameter tuning
doc2vec_fit_cv <- doc2vec_wflow %>%
fit_resamples(folds)
collect_metrics(doc2vec_fit_cv)
# A first fitting without any tuning or resampling
doc2vec_fit <- doc2vec_wflow %>%
fit(data = train_data)
# Testing the model fit on test data
predict(doc2vec_fit, test_data)
# Measuring the accuracy of the basic model with the augment function or predict, which requires further specifications and more actions
doc2vec_aug <-
augment(doc2vec_fit, test_data) %>%
select(doc_id, tag, .pred_class) %>%
mutate(
tag = factor(tag),
.pred_class = factor(.pred_class)
)
# doc2vec_pred <- predict(doc2vec_fit, test_data) %>%
#   bind_cols(predict(doc2vec_fit, test_data, type = "prob")) %>%
#   bind_cols(test_data %>% select(tag)) %>%
#   mutate(
#     tag = factor(tag)
#   )
# doc2vec_pred %>%
#   accuracy(truth = tag, .pred_class)
doc2vec_aug %>%
accuracy(truth = tag, .pred_class)
doc2vec_fit <- doc2vec_wflow %>%
fit(data = train_data)
# Testing the model fit on test data
predict(doc2vec_fit, test_data)
# Measuring the accuracy of the basic model with the augment function or predict, which requires further specifications and more actions
doc2vec_aug <-
augment(doc2vec_fit, test_data) %>%
select(doc_id, tag, .pred_class) %>%
mutate(
tag = factor(tag),
.pred_class = factor(.pred_class)
)
doc2vec_aug %>%
accuracy(truth = tag, .pred_class)
doc2vec_wflow <- workflow() %>%
add_model(svm_mod) %>%
add_recipe(doc2vec_rec)
doc2vec_rec <- recipe(tag ~ ., data = train_data) %>%
update_role(all_double(), new_role = "predictor") %>%
update_role(doc_id, new_role = "ID") %>%
update_role(tag, new_role = "outcome")
doc2vec_rec
# This creates a model object, in which you set the model specifications including the parameters to be tuned or the engine of the model
svm_mod <- svm_linear(cost = 0.01) %>%
set_engine("kernlab") %>%
set_mode("classification")
svm_mod
doc2vec_aug %>%
accuracy(truth = tag, .pred_class)
xfun::pkg_attach2("tidyverse", "ggplot2", "tidymodels")
load("data/doc2vec_df.RData")
# Fix the random numbers by setting the seed
# This enables the analysis to be reproducible when random numbers are used
set.seed(222)
# Put 3/4 of the data into the training set
data_split <- initial_split(doc2vec_df, prop = 3/4)
# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)
doc2vec_rec <- recipe(tag ~ ., data = train_data) %>%
update_role(all_double(), new_role = "predictor") %>%
update_role(doc_id, new_role = "ID") %>%
update_role(tag, new_role = "outcome")
doc2vec_rec
svm_mod <- svm_linear(cost = 0.01) %>%
set_engine("kernlab") %>%
set_mode("classification")
svm_mod
doc2vec_wflow <- workflow() %>%
add_model(svm_mod) %>%
add_recipe(doc2vec_rec)
doc2vec_fit <- doc2vec_wflow %>%
fit(data = train_data)
# Testing the model fit on test data
predict(doc2vec_fit, test_data)
# Measuring the accuracy of the basic model with the augment function or predict, which requires further specifications and more actions
doc2vec_aug <-
augment(doc2vec_fit, test_data) %>%
select(doc_id, tag, .pred_class) %>%
mutate(
tag = factor(tag),
.pred_class = factor(.pred_class)
)
doc2vec_aug %>%
accuracy(truth = tag, .pred_class)
load("data/doc2vec_df.RData")
doc2vec_df <- doc2vec_df %>% filter(dissenting_opinion == 1) %>% select(!dissenting_opinion)
xfun::pkg_attach2("tidyverse", "ggplot2", "tidymodels")
# Fix the random numbers by setting the seed
# This enables the analysis to be reproducible when random numbers are used
set.seed(222)
# Put 3/4 of the data into the training set
data_split <- initial_split(doc2vec_df, prop = 3/4)
# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)
doc2vec_rec <- recipe(tag ~ ., data = train_data) %>%
update_role(all_double(), new_role = "predictor") %>%
update_role(doc_id, new_role = "ID") %>%
update_role(tag, new_role = "outcome")
doc2vec_rec
# This creates a model object, in which you set the model specifications including the parameters to be tuned or the engine of the model
svm_mod <- svm_linear(cost = 0.01) %>%
set_engine("kernlab") %>%
set_mode("classification")
svm_mod
# Bind the model and the recipe to a workflow
doc2vec_wflow <- workflow() %>%
add_model(svm_mod) %>%
add_recipe(doc2vec_rec)
# A first fitting without any tuning or resampling
doc2vec_fit <- doc2vec_wflow %>%
fit(data = train_data)
predict(doc2vec_fit, test_data)
# Measuring the accuracy of the basic model with the augment function or predict, which requires further specifications and more actions
doc2vec_aug <-
augment(doc2vec_fit, test_data) %>%
select(doc_id, tag, .pred_class) %>%
mutate(
tag = factor(tag),
.pred_class = factor(.pred_class)
)
doc2vec_aug %>%
accuracy(truth = tag, .pred_class)
doc2vec_aug %>%
conf_mat(truth = tag, .pred_class)
source("~/Library/CloudStorage/OneDrive-Humboldt-UniversitaetzuBerlin,CMS/Programming/apex_courts_dataset/judgment_partitioning_tidymodeling.R", echo=TRUE)
xfun::pkg_attach2("tidyverse", "ggplot2", "tidymodels")
load("data/doc2vec_df.RData")
View(doc2vec_df)
doc2vec_df <- doc2vec_df %>% filter(tag != "dissent") %>% select(!dissenting_opinion)
# Fix the random numbers by setting the seed
# This enables the analysis to be reproducible when random numbers are used
set.seed(222)
# Put 3/4 of the data into the training set
data_split <- initial_split(doc2vec_df, prop = 3/4)
# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)
# This creates a recipe object. A recipe object contains the formula for the model as well as additional information for example on the role of the columns in a dataframe (ID, predictor, outcome)
doc2vec_rec <- recipe(tag ~ ., data = train_data) %>%
update_role(all_double(), new_role = "predictor") %>%
update_role(doc_id, new_role = "ID") %>%
update_role(tag, new_role = "outcome")
doc2vec_rec
# This creates a model object, in which you set the model specifications including the parameters to be tuned or the engine of the model
svm_mod <- svm_linear(cost = 0.01) %>%
set_engine("kernlab") %>%
set_mode("classification")
svm_mod
# Bind the model and the recipe to a workflow
doc2vec_wflow <- workflow() %>%
add_model(svm_mod) %>%
add_recipe(doc2vec_rec)
# A first fitting without any tuning or resampling
doc2vec_fit <- doc2vec_wflow %>%
fit(data = train_data)
# Testing the model fit on test data
predict(doc2vec_fit, test_data)
# Measuring the accuracy of the basic model with the augment function or predict, which requires further specifications and more actions
doc2vec_aug <-
augment(doc2vec_fit, test_data) %>%
select(doc_id, tag, .pred_class) %>%
mutate(
tag = factor(tag),
.pred_class = factor(.pred_class)
)
# doc2vec_pred <- predict(doc2vec_fit, test_data) %>%
#   bind_cols(predict(doc2vec_fit, test_data, type = "prob")) %>%
#   bind_cols(test_data %>% select(tag)) %>%
#   mutate(
#     tag = factor(tag)
#   )
# doc2vec_pred %>%
#   accuracy(truth = tag, .pred_class)
doc2vec_aug %>%
accuracy(truth = tag, .pred_class)
?case_when
?case_when
