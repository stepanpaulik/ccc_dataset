save(NSS_judges, file = "data/NSS_judges.RData")
?saveRDS
saveRDS(NSS_judges, file = "data/NSS_judges.Rds")
saveRDS(NSS_judges, file = "data/NSS_judges.rds")
load("data/US_metadata.RData")
xfun::pkg_attach2("udpipe","tidyverse", "RMySQL", "lubridate", "utf8", "uchardet")
US_metadata
US_metadata <- US_metadata %>% mutate(~replace(., is_empty(), NA))
US_metadata <- US_metadata %>% mutate(~replace(., is_empty(.), NA))
View(US_metadata)
US_metadata <- US_metadata %>% mutate(~replace(., is_empty(.), NA))
US_metadata <- US_metadata %>% mutate_all(~replace(., is_empty(.), NA))
View(US_metadata)
US_metadata <- US_metadata %>% mutate_all(~replace(., . == "", NA))
View(US_metadata)
# Save data
saveRDS(US_metadata, file = "data/US_metadata.rds")
loadRDS("data/US_metadata.rds")
loadRDS("data/US_metadata.rds")
?saveRDS
readRDS("data/US_metadata.rds")
US_metadata = readRDS("data/US_metadata.rds")
load("data/US_judges.RData")
load("data/US_texts.RData")
# Load data
load("data/US_compositions.RData")
saveRDS(US_compositions, file = "data/US_compositions.rds")
load("data/US_judges.RData")
saveRDS(US_judges, file = "data/US_judges.rds")
saveRDS(US_judges, file = "data/US_judges.rds")
US_judges = readRDS("data/US_judges.rds")
load("data/US_dissents.RData")
saveRDS(US_dissents, file = "data/US_dissents.rds")
US_dissents = readRDS("data/US_dissents.rds")
US_compositions = readRDS("data/US_compositions.rds")
US_texts = readRDS("data/US_texts.rds")
load(data/US_texts.Rdata)
load(data/US_texts.RData)
load("data/US_texts.RData")
saveRDS(US_texts, file = "data/US_texts.rds")
US_compositions = readRDS("data/US_compositions.rds")
US_texts = readRDS("data/US_texts.rds")
US_metadata = readRDS("data/US_metadata.rds")
US_judges = readRDS("data/US_judges.rds")
US_dissents = readRDS("data/US_dissents.rds")
View(US_metadata)
View(US_texts)
load("data/US_citations.RData")
# Save data
saveRDS("data/US_citations.rds")
# Save data
saveRDS(file = US_citations, "data/US_citations.rds")
# Save data
saveRDS(file = US_citations, "data/US_citations.rds")
# Save data
saveRDS(US_citations, "data/US_citations.rds")
save.image(file = "data/US_dataset.RData")
load("data/US_dataset.RData")
install.packages(c("class", "KernSmooth", "MASS", "nnet"))
xfun::pkg_attach2("tidyverse", "RMariaDB", "foreach")
# SQL commmunication
# Connect to the DB
conn <- dbConnect(
RMariaDB::MariaDB(),
dbname = "dataset_apexcourts",
username = "root",
password = "4E5ad7d!",
host = "localhost",
port = 3306
)
dbExecute(conn, "SET NAMES 'utf8'")
writeNewDb <- function(data, table) {
dbWriteTable(
conn,
table,
data,
overwrite = TRUE,
row.names = FALSE
)
}
US_metadata = readRDS("data/US_metadata.rds")
# Write the dataframes as tables in MySQL database
US_metadata %>% writeNewDb(data = ., table = "US_metadata")
remove(US_metadata)
US_metadata = tbl(conn, "US_metadata")
US_metadata = tbl(conn, "US_metadata")
?copy_to
US_metadata = readRDS("data/US_metadata.rds")
copy_to(conn, US_metadata)
copy_to(conn, US_metadata)
remove(US_metadata)
US_metadata = tbl(conn, "US_metadata")
US_metadata = readRDS("data/US_metadata.rds")
# Write the dataframes as tables in MySQL database
US_metadata %>% writeNewDb(data = ., table = "US_metadata")
?dbConnect
# SQL commmunication
# Connect to the DB
conn <- dbConnect(
RMariaDB::MariaDB(),
":memory:"
dbname = "dataset_apexcourts",
# SQL commmunication
# Connect to the DB
conn <- dbConnect(
RMariaDB::MariaDB(),
":memory:",
dbname = "dataset_apexcourts",
username = "root",
password = "4E5ad7d!",
host = "localhost",
port = 3306
)
# SQL commmunication
# Connect to the DB
conn <- dbConnect(
RMariaDB::MariaDB(),
dbname = "dataset_apexcourts",
username = "root",
password = "4E5ad7d!",
host = "localhost",
port = 3306
)
# SQL commmunication
# Connect to the DB
conn <- dbConnect(
RMariaDB::MariaDB(),
dbname = "dataset_apexcourts",
username = "root",
password = "4E5ad7d!",
host = "localhost",
port = 3306
)
?tbl
copy_to(conn, "US_metadata")
US_metadata = tbl(conn, "US_metadata")
US_metadata <- tbl(conn, "US_metadata")
src_dbi(conn)
src(conn)
src_dbi(connn)
xfun::pkg_attach2("tidyverse", "RMariaDB", "foreach")
src_dbi(connn)
DBI::src_dbi(connn)
US_metadata = tbl(conn, "US_metadata")
US_metadata = tbl(conn, "US_metadata") %>% as.tibble()
View(US_metadata)
US_metadata
US_metadata = tbl(conn, "US_metadata")
US_metadata
US_metadata %>%
group_by(judge_rapporteur, year_decision) %>%
summarise(count = n()) %>%
show_query()
US_metadata %>%
group_by(judge_rapporteur, year_decision) %>%
summarise(count = n())
US_metadata = US_metadata %>%
group_by(judge_rapporteur, year_decision) %>%
summarise(count = n())
US_metadata
US_metadata = tbl(conn, "US_metadata")
query = US_metadata %>%
group_by(judge_rapporteur, year_decision) %>%
summarise(count = n()) %>%
show_query()
query = US_metadata %>%
group_by(judge_rapporteur, year_decision) %>%
summarise(count = n()) %>%
show_query() %>%
capture.output()
query <- query[2:length(query)]
?gsub
query = US_metadata %>%
group_by(judge_rapporteur, year_decision) %>%
summarise(count = n()) %>%
show_query() %>%
capture.output()
US_metadata = tbl(conn, "US_metadata")
query = US_metadata %>%
group_by(judge_rapporteur, year_decision) %>%
summarise(count = n()) %>%
show_query() %>%
capture.output()
query[2]
query <- query[2:length(query)] %>% str_remove_all(pattern = "`")
dbSendQuery(conn, query)
query <- query[2:length(query)] %>% str_remove_all(pattern = "`") %>% paste()
query
query = US_metadata %>%
group_by(judge_rapporteur, year_decision) %>%
summarise(count = n()) %>%
show_query() %>%
capture.output()
query <- query[2:length(query)] %>% str_remove_all(pattern = "`")
tmp <- tempfile()
writeLines(query,tmp)
writeLines(query,tmp)
tmp
writeLines(query, tmp)
?collect
?compute
query = US_metadata %>%
group_by(judge_rapporteur, year_decision) %>%
summarise(count = n()) %>%
compute(temporary = FALSE, name = "US_metadata")
query = US_metadata %>%
group_by(judge_rapporteur, year_decision) %>%
summarise(count = n()) %>%
compute(temporary = FALSE, name = "US_metadata", overwrite = TRUE)
dbExecute
?dbExecute
xfun::pkg_attach2("tidyverse", "ggplot2", "tidymodels")
load("data/doc2vec_df.RData")
# Random data filtering
doc2vec_df <- doc2vec_df %>% filter(tag != "dissent") %>% select(!dissenting_opinion)
# Fix the random numbers by setting the seed
# This enables the analysis to be reproducible when random numbers are used
set.seed(222)
# Put 3/4 of the data into the training set
data_split <- initial_split(doc2vec_df, prop = 3/4)
# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)
doc2vec_rec <- recipe(tag ~ ., data = train_data) %>%
update_role(all_double(), new_role = "predictor") %>%
update_role(doc_id, new_role = "ID") %>%
update_role(tag, new_role = "outcome")
doc2vec_rec
# This creates a model object, in which you set the model specifications including the parameters to be tuned or the engine of the model
svm_mod <- svm_linear(cost = tune()) %>%
set_engine("kernlab") %>%
set_mode("classification")
svm_mod
# Bind the model and the recipe to a workflow
doc2vec_wflow <- workflow() %>%
add_model(svm_mod) %>%
add_recipe(doc2vec_rec)
# A first fitting without any tuning or resampling
doc2vec_fit <- doc2vec_wflow %>%
fit(data = train_data)
# A first fitting without any tuning or resampling
doc2vec_fit <- doc2vec_wflow %>%
fit(data = train_data)
xfun::pkg_attach2("tidyverse", "ggplot2", "tidymodels", "kernlab")
# A first fitting without any tuning or resampling
doc2vec_fit <- doc2vec_wflow %>%
fit(data = train_data)
# A first fitting without any tuning or resampling
doc2vec_fit <- doc2vec_wflow %>%
fit(data = train_data)
# A first fitting without any tuning or resampling
doc2vec_fit <- doc2vec_wflow %>%
fit(data = train_data)
View(doc2vec_df)
doc2vec_rec <- recipe(tag ~ ., data = train_data) %>%
update_role(all_double(), new_role = "predictor") %>%
update_role(doc_id, new_role = "ID") %>%
update_role(tag, new_role = "outcome")
doc2vec_rec
# This creates a model object, in which you set the model specifications including the parameters with the tuned values or the engine of the model
svm_mod <- svm_linear(cost = 0.01) %>%
set_engine("kernlab") %>%
set_mode("classification")
svm_mod
# Bind the model and the recipe to a workflow
doc2vec_wflow <- workflow() %>%
add_model(svm_mod) %>%
add_recipe(doc2vec_rec)
doc2vec_fit <- doc2vec_wflow %>%
fit(data = train_data)
doc2vec_fit <- doc2vec_wflow %>%
fit(data = train_data)
# Testing the model fit on test data
predict(doc2vec_fit, test_data)
# Measuring the accuracy of the basic model with the augment function or predict, which requires further specifications and more actions
doc2vec_aug <-
augment(doc2vec_fit, test_data) %>%
select(doc_id, tag, .pred_class) %>%
mutate(
tag = factor(tag),
.pred_class = factor(.pred_class)
)
doc2vec_aug %>%
accuracy(truth = tag, .pred_class)
doc2vec_aug %>%
conf_mat(truth = tag, .pred_class)
?boost_tree
xfun::pkg_attach2("tidyverse", "ggplot2", "tidymodels", "kernlab", "xgboost")
doc2vec_aug %>%
accuracy(truth = tag, .pred_class)
load("data/doc2vec_df.RData")
saveRDS(doc2vec_df, "data/doc2vec_df.rds")
doc2vec_rec <- recipe(tag ~ ., data = train_data) %>%
update_role(all_double(), new_role = "predictor") %>%
update_role(doc_id, new_role = "ID") %>%
update_role(tag, new_role = "outcome")
doc2vec_rec
## Boosted trees via xgboost
xgboost_mod <- boost_tree(
mtry = integer(), trees = integer(), min_n = integer(), tree_depth = integer(),
learn_rate = numeric(), loss_reduction = numeric(), sample_size = numeric(),
stop_iter = integer()
) %>%
set_engine("xgboost") %>%
set_mode("classification")
boost_tree(
mtry = integer(), trees = integer(), min_n = integer(), tree_depth = integer(),
learn_rate = numeric(), loss_reduction = numeric(), sample_size = numeric(),
stop_iter = integer()
) %>%
set_engine("xgboost") %>%
set_mode("classification") %>% translate()
?xgb_train
## Boosted trees via xgboost
xgboost_mod <- boost_tree(
trees = 1000,
tree_depth = tune(), min_n = tune(),
loss_reduction = tune(),                     ## first three: model complexity
sample_size = tune(), mtry = tune(),         ## randomness
learn_rate = tune()                          ## step size
) %>%
set_engine("xgboost") %>%
set_mode("classification")
doc2vec_wflow_xgboost <-
add_model(xgboost_mod) %>%
add_recipe(doc2vec_rec)
## Boosted trees via xgboost
xgboost_mod <- boost_tree(
trees = 1000,
tree_depth = tune(), min_n = tune(),
loss_reduction = tune(),
sample_size = tune(), mtry = tune(),
learn_rate = tune()
) %>%
set_engine("xgboost") %>%
set_mode("classification")
doc2vec_wflow_xgboost <-
add_model(xgboost_mod) %>%
add_recipe(doc2vec_rec)
xfun::pkg_attach2("tidyverse", "ggplot2", "tidymodels", "kernlab", "xgboost")
doc2vec_df <- readRDS("data/doc2vec_df.rds")
# Fix the random numbers by setting the seed
# This enables the analysis to be reproducible when random numbers are used
set.seed(222)
# Put 3/4 of the data into the training set
data_split <- initial_split(doc2vec_df, prop = 3/4)
# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)
# Tuning
# This creates a recipe object. A recipe object contains the formula for the model as well as additional information for example on the role of the columns in a dataframe (ID, predictor, outcome)
doc2vec_rec <- recipe(tag ~ ., data = train_data) %>%
update_role(all_double(), new_role = "predictor") %>%
update_role(doc_id, new_role = "ID") %>%
update_role(tag, new_role = "outcome")
doc2vec_rec
?boost_tree
## Boosted trees via xgboost
xgboost_mod <- boost_tree(
trees = 1000,
tree_depth = tune(), min_n = tune(),
loss_reduction = tune(),
sample_size = tune(), mtry = tune(),
learn_rate = tune()
) %>%
set_engine("xgboost") %>%
set_mode("classification")
doc2vec_wflow_xgboost <-
add_model(xgboost_mod) %>%
add_recipe(doc2vec_rec)
xgboost_mod
xgb_grid <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), vb_train),
learn_rate(),
size = 30
)
doc2vec_wflow_xgboost <- workflow() %>%
add_model(xgboost_mod) %>%
add_recipe(doc2vec_rec)
folds <- vfold_cv(train_data, v = 6)
xfun::pkg_attach2("tidyverse", "ggplot2", "tidymodels", "kernlab", "xgboost", "doParallel")
# The tuning grif for the boosted tree model
xgboost_grid <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), vb_train),
learn_rate(),
size = 30
)
doParallel::registerDoParallel()
# Create the workflow
xboost_wflow_xgboost <- workflow() %>%
add_model(xgboost_mod) %>%
add_recipe(doc2vec_rec)
# Create the workflow
xboost_wflow <- workflow() %>%
add_model(xgboost_mod) %>%
add_recipe(doc2vec_rec)
xfun::pkg_attach2("tidyverse", "ggplot2", "tidymodels", "kernlab", "xgboost", "doParallel")
doc2vec_df <- readRDS("data/doc2vec_df.rds")
# Fix the random numbers by setting the seed
# This enables the analysis to be reproducible when random numbers are used
set.seed(222)
# Put 3/4 of the data into the training set
data_split <- initial_split(doc2vec_df, prop = 3/4)
# Create data frames for the two sets:
train_data <- training(data_split)
test_data  <- testing(data_split)
xgboost_rec <- recipe(tag ~ ., data = train_data) %>%
update_role(all_double(), new_role = "predictor") %>%
update_role(doc_id, new_role = "ID") %>%
update_role(tag, new_role = "outcome")
xgboost_rec
# The modle
xgboost_mod <- boost_tree(
trees = 1000,
tree_depth = tune(), min_n = tune(),
loss_reduction = tune(),
sample_size = tune(), mtry = tune(),
learn_rate = tune()
) %>%
set_engine("xgboost") %>%
set_mode("classification")
xgboost_mod
# The tuning grif for the boosted tree model
xgboost_grid <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), vb_train),
learn_rate(),
size = 30
)
)
# The tuning grif for the boosted tree model
xgboost_grid <- grid_latin_hypercube(
tree_depth(),
min_n(),
loss_reduction(),
sample_size = sample_prop(),
finalize(mtry(), train_data),
learn_rate(),
size = 30
)
# Create the workflow
xboost_wflow <- workflow() %>%
add_model(xgboost_mod) %>%
add_recipe(xgboost_rec)
# Again create crossvalidation data
folds <- vfold_cv(train_data, v = 6)
doParallel::registerDoParallel()
xgboost_res <- tune_grid(
xgboost_wf,
resamples = vb_folds,
grid = xgb_grid,
control = control_grid(save_pred = TRUE)
)
xgboost_res <- tune_grid(
xgboost_wflow,
resamples = vb_folds,
grid = xgb_grid,
control = control_grid(save_pred = TRUE)
)
xgboost_res <- tune_grid(
xgboost_wflow,
resamples = vb_folds,
grid = xgb_grid,
control = control_grid(save_pred = TRUE)
)
# Create the workflow
xboost_wflow <- workflow() %>%
add_model(xgboost_mod) %>%
add_recipe(xgboost_rec)
# Create the workflow
xgboost_wflow <- workflow() %>%
add_model(xgboost_mod) %>%
add_recipe(xgboost_rec)
xgboost_res <- tune_grid(
xgboost_wflow,
resamples = vb_folds,
grid = xgb_grid,
control = control_grid(save_pred = TRUE)
)
xgboost_res <- tune_grid(
xgboost_wflow,
resamples = folds,
grid = xgboost_grid,
control = control_grid(save_pred = TRUE)
)
xfun::pkg_attach2("udpipe","tidyverse", "utf8", "uchardet")
# Load data
US_metadata = readRDS("../data/US_metadata.rds")
US_metadata$applicant %<>% modify(.x = ., ~ str_replace_all(string = .x, pattern = "POSTĚŽOVATEL", replacement = "PO, STĚŽOVATEL"))
US_metadata$applicant %<>% modify(.x = ., ~ str_replace_all(string = .x, pattern = "POSTĚŽOVATEL", replacement = "PO, STĚŽOVATEL")) %>% str_trim()
View(US_metadata)
unique(US_metadata$applicant)
US_metadata$applicant %<>% modify(.x = ., ~ str_replace_all(string = .x, pattern = "FOSTĚŽOVATEL", replacement = "FO, STĚŽOVATEL")) %>% str_trim()
unique(US_metadata$applicant)
US_metadata$applicant %<>% modify(.x = ., ~ str_replace_all(string = .x, pattern = "STRANAPOLITICKÁ", replacement = "STRANA, POLITICKÁ")) %>% str_trim()
unique(US_metadata$applicant)
?x
?grepl
US_metadata$applicant %<>% modify_if(.x = ., .p = grepl(pattern = "[A-Za-z]STĚŽOVATEL", x = .x), ~ str_replace_all(string = .x, pattern = "STĚŽOVATEL", replacement = ", STĚŽOVATEL")) %>% str_trim()
?str_find_all
US_metadata$applicant %<>% modify_if(.x = ., .p = grepl(pattern = "[A-Za-z]STĚŽOVATEL", x = .x), ~ str_replace_all(string = .x, pattern = "STĚŽOVATEL", replacement = ", STĚŽOVATEL")) %>% str_trim()
